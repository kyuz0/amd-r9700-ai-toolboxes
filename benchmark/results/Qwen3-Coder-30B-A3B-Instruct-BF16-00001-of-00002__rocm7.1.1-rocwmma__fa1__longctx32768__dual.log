ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 2 ROCm devices:
  Device 0: AMD Radeon AI PRO R9700, gfx1201 (0x1201), VMM: no, Wave Size: 32
  Device 1: AMD Radeon AI PRO R9700, gfx1201 (0x1201), VMM: no, Wave Size: 32
| model                          |       size |     params | backend    | ngl | n_ubatch | fa |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: |
/opt/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:94: ROCm error
/usr/local/lib64/libggml-base.so.0(+0x35a5) [0x7fd6fceb05a5]
/usr/local/lib64/libggml-base.so.0(ggml_print_backtrace+0x1eb) [0x7fd6fceb096b]
/usr/local/lib64/libggml-base.so.0(ggml_abort+0x11f) [0x7fd6fceb0aef]
/usr/local/lib64/libggml-hip.so.0(+0x2fcfa42) [0x7fd6fff3ca42]
/usr/local/lib64/libggml-hip.so.0(+0x2fe4297) [0x7fd6fff51297]
/usr/local/lib64/libggml-hip.so.0(+0x2fd92ea) [0x7fd6fff462ea]
/usr/local/lib64/libggml-hip.so.0(+0x2fd4c8f) [0x7fd6fff41c8f]
/usr/local/lib64/libggml-base.so.0(ggml_backend_sched_graph_compute_async+0x7f3) [0x7fd6fcecb483]
/usr/local/lib64/libllama.so.0(_ZN13llama_context13graph_computeEP11ggml_cgraphb+0xa0) [0x7fd7006137e0]
/usr/local/lib64/libllama.so.0(_ZN13llama_context14process_ubatchERK12llama_ubatch14llm_graph_typeP22llama_memory_context_iR11ggml_status+0xe2) [0x7fd7006152b2]
/usr/local/lib64/libllama.so.0(_ZN13llama_context6decodeERK11llama_batch+0x3bf) [0x7fd70061a6ff]
/usr/local/lib64/libllama.so.0(llama_decode+0xe) [0x7fd70061b4fe]
/usr/local/bin/llama-bench() [0x40ad9b]
/usr/local/bin/llama-bench() [0x4088ac]
/lib64/libc.so.6(+0x35b5) [0x7fd6fc8465b5]
/lib64/libc.so.6(__libc_start_main+0x88) [0x7fd6fc846668]
/usr/local/bin/llama-bench() [0x409c25]
âœ– ! [rocm7.1.1-rocwmma] Qwen3-Coder-30B-A3B-Instruct-BF16-00001-of-00002__fa1 __longctx32768 failed (exit 0)
